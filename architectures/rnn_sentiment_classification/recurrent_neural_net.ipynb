{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aeb6702-e40f-4111-9c4d-0e02150a0dfd",
   "metadata": {},
   "source": [
    "# Implementing a Recurrent Neural Network (RNN) from Scratch in Python\n",
    "\n",
    "### Overview\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks that excel in processing sequential data. They are a fundamental building block in the field of natural language processing (NLP) and are also widely used in other applications such as time-series analysis, speech recognition, and music generation. The ability of RNNs to maintain a 'memory' of previous inputs makes them uniquely suited for tasks where context and order are crucial.\n",
    "\n",
    "### Goal\n",
    "\n",
    "In this notebook, I manually implement a basic RNN using only NumPy to deepen my understanding of its mechanics. This will serve as an exercise to solidify my theoretical knowledge and also improve my practical skills in building neural networks from the ground up. Throughout this project, I will include detailed markdown cells explaining different components of the architecture so that this notebook can serve as a comprehensive reference point in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc307b6b-f0b0-46f2-a4b1-71a887d53d49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T04:16:54.911143Z",
     "start_time": "2024-05-23T04:16:54.105665Z"
    }
   },
   "source": [
    "#Basic Scientific Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Scikit-learn for \n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "9a1d2a90-1694-4ceb-a929-51d062301073",
   "metadata": {},
   "source": [
    "## Gentle Introduction \n",
    "\n",
    "### Understanding Sequential Data\n",
    "\n",
    "**Sequential data** refers to any data where the order matters. This can be anything from a sentence in a text (where the sequence of words affects the meaning) to a series of stock prices (where past prices can influence future prices). \n",
    "\n",
    "To illustrate handling sequential data in machine learning, specifically with Recurrent Neural Networks (RNNs), consider a sentence containing $n$ words. In an RNN model, this sentence can be represented as a sequence of $n$ word embeddings:\n",
    "\n",
    "- $X_{0}$ represents the word embedding of the first word,\n",
    "- $X_{1}$ represents the word embedding of the second word,\n",
    "- ...,\n",
    "- $X_{n-1}$ represents the word embedding of the $n$th word.\n",
    "\n",
    "It's important to note the zero-based indexing used here: $X_{0}$ is the first word's embedding, $X_{1}$ the second, and so on up to $X_{n-1}$, which is the $n$th word's embedding.\n",
    "\n",
    "### How RNNs Leverage Sequential Data\n",
    "\n",
    "RNNs are particularly well-suited for processing this type of data because they maintain an internal state that gets updated as they process each word in a sequence. This mechanism allows them to consider not just the current word's information (embedded in $X_t$), but also the context provided by all previous words in the sequence.\n",
    "\n",
    "For instance, when predicting the next word in a sentence after the second word $X_1$, the RNN uses both the information in $X_1$ and the context accumulated from processing the first word $X_0$. This ability to carry information across different time steps enables RNNs to make more informed predictions and capture dependencies that span across the sequence. Let's take a look at what this looks like mathematically:\n",
    "\n",
    "1. **State Update Equation**:\n",
    "   At each step in the sequence, the RNN updates its state by combining information from the current input and the previous state. This updated state is computed using:\n",
    "   $$\n",
    "   h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)\n",
    "   $$\n",
    "   where:\n",
    "   - $h_{t-1}$ is the previous state,\n",
    "   - $x_t$ is the current input,\n",
    "   - $W_{hh}$ and $W_{xh}$ are weights that determine how the previous state and the current input respectively influence the new state,\n",
    "   - $b_h$ is a bias term,\n",
    "   - $\\tanh$ is a nonlinear function that helps model complex patterns.\n",
    "\n",
    "2. **Output Generation**:\n",
    "   The output at each step is generated based on the current state:\n",
    "   $$\n",
    "   o_t = W_{hy} h_t + b_y\n",
    "   $$\n",
    "   where $W_{hy}$ is the weight matrix connecting the state to the output and $b_y$ is the output bias.\n",
    "\n",
    "\n",
    "We shall implement these functions in Python below."
   ]
  },
  {
   "cell_type": "code",
   "id": "66429181-56d4-4582-979e-7180315b84bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T03:35:06.837922Z",
     "start_time": "2024-05-23T03:35:06.834827Z"
    }
   },
   "source": [
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "461a7258-0c8b-4b35-bc75-38c403b419f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T03:35:10.469646Z",
     "start_time": "2024-05-23T03:35:10.465758Z"
    }
   },
   "source": [
    "def update_hidden_state(h_prev, x_t, W_hh, W_xh, b_h):\n",
    "    \"\"\"\n",
    "    Update the hidden state for a recurrent neural network.\n",
    "\n",
    "    Parameters:\n",
    "    h_prev (numpy.ndarray): The previous hidden state (vector), shape (hidden_size, 1)\n",
    "    x_t (numpy.ndarray): The current input (vector), shape (input_size, 1)\n",
    "    W_hh (numpy.ndarray): Weight matrix for the hidden-to-hidden connection, shape (hidden_size, hidden_size)\n",
    "    W_xh (numpy.ndarray): Weight matrix for the input-to-hidden connection, shape (hidden_size, input_size)\n",
    "    b_h (numpy.ndarray): Bias vector for the hidden state, shape (hidden_size, 1)\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The updated hidden state (vector), shape (hidden_size, 1)\n",
    "    \"\"\"\n",
    "    z = np.dot(W_hh, h_prev) + np.dot(W_xh, x_t) + b_h\n",
    "    h_t = tanh(z)\n",
    "    return h_t"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "02fa5ae4-24b2-4723-bf08-a582c8ce4e36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T03:35:12.786850Z",
     "start_time": "2024-05-23T03:35:12.784321Z"
    }
   },
   "source": [
    "def calculate_step_output(W_hy, h_t, b_y):\n",
    "    return np.dot(W_hy, h_t) + b_y"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "48bb9141-166d-4d27-90c4-39b2d7338edb",
   "metadata": {},
   "source": [
    "### Further Intuition\n",
    "\n",
    "Before we continue on, let's break down what's going on in the state update equation. The $W_{hh} h_{t-1}$ term, which is the linear combination of the weights between hidden states and the previous state, tells us how much the previous state contributes to our current output.\n",
    "\n",
    "1. **$W_{hh} h_{t-1}$**: This term represents the influence of the previous hidden state on the current state. Here:\n",
    "   - $W_{hh}$ is a weight matrix that captures the relationship between the previous hidden state and the current one. It is crucial for the network to remember or carry forward information from the past.\n",
    "   - $h_{t-1}$ is the hidden state from the previous timestep. It holds information that the network has processed up to that point.\n",
    "   \n",
    "   The product $W_{hh} h_{t-1}$ computes how much of the past information (from the previous timestep) should be retained or modified. It can be seen as the network's memory influencing its current decision or state.\n",
    "\n",
    "2. **$W_{xh} x_t$**: This term deals with the current input:\n",
    "   - $W_{xh}$ is a weight matrix that connects the input layer to the hidden layer.\n",
    "   - $x_t$ is the input at the current timestep.\n",
    "\n",
    "   The product $W_{xh} x_t$ represents how the current input contributes to the new state. It's crucial for integrating new information into the network.\n",
    "\n",
    "3. **$b_h$**: This is the bias term for the hidden layer. It provides additional flexibility to the model, allowing it to better fit the data by adjusting the output independently of the weighted inputs. Biases help to shift the activation function to either the left or right, which can be critical for learning patterns.\n",
    "\n",
    "4. **Activation Function $\\tanh$**: The hyperbolic tangent function, $\\tanh$, is used to add non-linearity to the process, helping the network learn complex patterns. It also squashes the output to be between -1 and 1, which helps in stabilizing the network by controlling the scale of output values.\n",
    "\n",
    "By combining these components, the RNN can maintain a balance between remembering past information and incorporating new inputs, thereby effectively handling sequential data. This equation is repeated at each timestep, continuously updating the hidden state throughout the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e09721-a066-4374-a141-f3b7b4590601",
   "metadata": {},
   "source": [
    "A **state** in the context of an RNN represents the memory of the network, encapsulating information about the inputs it has processed so far. In simple terms, the state helps the network 'remember' what it has seen in the sequence, which influences how it processes and reacts to new inputs.\n",
    "\n",
    "### Components of an RNN\n",
    "\n",
    "1. **Input Layer**: This is where the network receives the sequence of data one element at a time.\n",
    "2. **Hidden Layer (Recurrent Layer)**: This is the core of an RNN, where the inputs and previous states are combined to update the current state.\n",
    "3. **Output Layer**: Depending on the application, the network can produce an output at each step or only at the end of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351252da-6115-47e1-8e66-90fb14ca5dea",
   "metadata": {},
   "source": [
    "2. **Output Generation**:\n",
    "   The output at each step is generated based on the current state:\n",
    "   $$\n",
    "   o_t = W_{hy} h_t + b_y\n",
    "   $$\n",
    "   where $W_{hy}$ is the weight matrix connecting the state to the output and $b_y$ is the output bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8259188-5273-42ad-9a0c-cc1bacf20086",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "The parameters of the model (i.e., $W_{hh}$, $W_{xh}$, $W_{hy}$, $b_h$, and $b_y$) are learned through a process called backpropagation through time (BPTT). BPTT is an extension of the standard backpropagation algorithm used in feedforward networks, modified to handle the sequential nature of RNNs. It involves:\n",
    "\n",
    "- **Forward Pass**: Computing the predicted output and the hidden states for all time steps.\n",
    "- **Loss Calculation**: Evaluating the discrepancy between the predicted output and the actual target output using a loss function, typically Mean Squared Error (MSE) or Cross-Entropy Loss.\n",
    "- **Backward Pass**: Propagating the error back through the network to update the weights and biases to minimize the loss.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "Despite their flexibility and power, RNNs are often challenged by issues like:\n",
    "- **Vanishing Gradient Problem**: During backpropagation, gradients can vanish (become very small) as they are propagated back through time and layers, making learning long-term dependencies practically impossible.\n",
    "- **Exploding Gradient Problem**: Conversely, gradients can also explode (become very large), which may lead to diverging weights during training.\n",
    "\n",
    "To address these issues, more sophisticated variants of RNNs, such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), have been developed.\n",
    "\n",
    "This section has laid out the fundamental architecture of RNNs and their mathematical underpinnings, providing a foundation for understanding how they operate and are implemented in practical applications."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T04:48:03.413253Z",
     "start_time": "2024-05-23T04:48:02.721679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reviews_df = pd.read_csv('data/preprocessed_movie_reviews.csv')\n",
    "reviews_df"
   ],
   "id": "d517e480aec81234",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                              embeddings  sentiment_label  \n",
       "0      [ 1.08621784e-01  4.46074829e-02 -7.82461986e-...                1  \n",
       "1      [ 0.08574718  0.2102892  -0.3049311  -0.184755...                1  \n",
       "2      [ 0.15635169  0.12138394 -0.12073819 -0.237573...                1  \n",
       "3      [ 0.16219215  0.02081418 -0.1491204  -0.251864...                0  \n",
       "4      [ 0.28672162  0.26971683 -0.09641971 -0.072171...                1  \n",
       "...                                                  ...              ...  \n",
       "49995  [ 0.12475395  0.09159596 -0.00999248 -0.298130...                1  \n",
       "49996  [ 3.33226664e-04 -8.18859190e-02 -6.32197186e-...                0  \n",
       "49997  [ 0.13239665  0.2664471  -0.29731384 -0.560151...                0  \n",
       "49998  [ 5.8087081e-02 -5.2524090e-02 -1.5382867e-01 ...                0  \n",
       "49999  [ 0.3054847   0.15439945  0.13942745 -0.084910...                0  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[ 1.08621784e-01  4.46074829e-02 -7.82461986e-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[ 0.08574718  0.2102892  -0.3049311  -0.184755...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[ 0.15635169  0.12138394 -0.12073819 -0.237573...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 0.16219215  0.02081418 -0.1491204  -0.251864...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[ 0.28672162  0.26971683 -0.09641971 -0.072171...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[ 0.12475395  0.09159596 -0.00999248 -0.298130...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 3.33226664e-04 -8.18859190e-02 -6.32197186e-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 0.13239665  0.2664471  -0.29731384 -0.560151...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 5.8087081e-02 -5.2524090e-02 -1.5382867e-01 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[ 0.3054847   0.15439945  0.13942745 -0.084910...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T04:54:05.565047Z",
     "start_time": "2024-05-23T04:54:05.541114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = list(reviews_df['embeddings'])  # Assuming embeddings are stored in a way that each row is a valid input for models\n",
    "y = reviews_df['sentiment_label'].values"
   ],
   "id": "086d8e26-c9fb-4d0c-a51e-ce95918ec230",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T04:54:20.420970Z",
     "start_time": "2024-05-23T04:54:20.396618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% training and 20% testing"
   ],
   "id": "3bac6faa3d3c8632",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T04:55:01.532341Z",
     "start_time": "2024-05-23T04:55:01.526418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Training data features shape: {len(X_train)}')\n",
    "print(f'Training data labels shape: {len(y_train)}')\n",
    "print(f'Testing data features shape: {len(X_test)}')\n",
    "print(f'Testing data labels shape: {len(y_test)}')"
   ],
   "id": "39a64b138b84132c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data features shape: 40000\n",
      "Training data labels shape: 40000\n",
      "Testing data features shape: 10000\n",
      "Testing data labels shape: 10000\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "h_prev = np.zeros((300, 1))\n",
    "for i, embedding in enumerate(sequence):\n",
    "    \n",
    "    x_t = embedding\n",
    "    hidden_state = update_hidden_state()"
   ],
   "id": "b11b3fc1729bc50f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T04:55:45.323998Z",
     "start_time": "2024-05-23T04:55:45.319786Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c52b215809af554a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aababbcad33cf4d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

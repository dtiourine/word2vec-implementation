{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aeb6702-e40f-4111-9c4d-0e02150a0dfd",
   "metadata": {},
   "source": [
    "# Implementing a Recurrent Neural Network (RNN) from Scratch in Python\n",
    "\n",
    "## Overview\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks that excel in processing sequential data. They are a fundamental building block in the field of natural language processing (NLP) and are also widely used in other applications such as time-series analysis, speech recognition, and music generation. The ability of RNNs to maintain a 'memory' of previous inputs makes them uniquely suited for tasks where context and order are crucial.\n",
    "\n",
    "## Goal\n",
    "\n",
    "In this notebook, I manually implement a basic RNN using only NumPy to deepen my understanding of its mechanics. This will serve as an exercise to solidify my theoretical knowledge and also improve my practical skills in building neural networks from the ground up. Throughout this project, I will include detailed markdown cells explaining different components of the architecture so that this notebook can serve as a comprehensive reference point in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc307b6b-f0b0-46f2-a4b1-71a887d53d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d2a90-1694-4ceb-a929-51d062301073",
   "metadata": {},
   "source": [
    "# Architecture Overview\n",
    "\n",
    "## Introduction to Recurrent Neural Networks (RNN)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks specifically designed to handle sequential data. Unlike standard feedforward neural networks, RNNs have the distinctive feature of maintaining a form of internal memory or state, which helps them to process sequences of inputs over time.\n",
    "\n",
    "### What are Sequential Data and States?\n",
    "\n",
    "**Sequential data** refers to any data where the order matters. This can be anything from a sentence in a text (where the sequence of words affects the meaning) to a series of stock prices (where past prices can influence future prices). \n",
    "\n",
    "A **state** in the context of an RNN represents the memory of the network, encapsulating information about the inputs it has processed so far. In simple terms, the state helps the network 'remember' what it has seen in the sequence, which influences how it processes and reacts to new inputs.\n",
    "\n",
    "### Components of an RNN\n",
    "\n",
    "1. **Input Layer**: This is where the network receives the sequence of data one element at a time.\n",
    "2. **Hidden Layer (Recurrent Layer)**: This is the core of an RNN, where the inputs and previous states are combined to update the current state.\n",
    "3. **Output Layer**: Depending on the application, the network can produce an output at each step or only at the end of the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb9141-166d-4d27-90c4-39b2d7338edb",
   "metadata": {},
   "source": [
    "### How does an RNN Work?\n",
    "\n",
    "The operations within an RNN can be broken down into steps that correspond to each element of the input sequence:\n",
    "\n",
    "1. **State Update Equation**:\n",
    "   At each step in the sequence, the RNN updates its state by combining information from the current input and the previous state. This updated state is computed using:\n",
    "   $$\n",
    "   h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)\n",
    "   $$\n",
    "   where:\n",
    "   - $h_{t-1}$ is the previous state,\n",
    "   - $x_t$ is the current input,\n",
    "   - $W_{hh}$ and $W_{xh}$ are weights that determine how the previous state and the current input respectively influence the new state,\n",
    "   - $b_h$ is a bias term,\n",
    "   - $\\tanh$ is a nonlinear function that helps model complex patterns.\n",
    "\n",
    "2. **Output Generation**:\n",
    "   The output at each step is generated based on the current state:\n",
    "   $$\n",
    "   o_t = W_{hy} h_t + b_y\n",
    "   $$\n",
    "   where $W_{hy}$ is the weight matrix connecting the state to the output and $b_y$ is the output bias.\n",
    "\n",
    "Before we continue on, let's break down what's going on this equation. The $W_{hh} h_{t-1}$ term, which is the linear combination of the weights between hidden states and the previous state, tells us how much the previous state contributes to our current output.\n",
    "\n",
    "1. **$W_{hh} h_{t-1}$**: This term represents the influence of the previous hidden state on the current state. Here:\n",
    "   - $W_{hh}$ is a weight matrix that captures the relationship between the previous hidden state and the current one. It is crucial for the network to remember or carry forward information from the past.\n",
    "   - $h_{t-1}$ is the hidden state from the previous timestep. It holds information that the network has processed up to that point.\n",
    "   \n",
    "   The product $W_{hh} h_{t-1}$ computes how much of the past information (from the previous timestep) should be retained or modified. It can be seen as the network's memory influencing its current decision or state.\n",
    "\n",
    "2. **$W_{xh} x_t$**: This term deals with the current input:\n",
    "   - $W_{xh}$ is a weight matrix that connects the input layer to the hidden layer.\n",
    "   - $x_t$ is the input at the current timestep.\n",
    "\n",
    "   The product $W_{xh} x_t$ represents how the current input contributes to the new state. It's crucial for integrating new information into the network.\n",
    "\n",
    "3. **$b_h$**: This is the bias term for the hidden layer. It provides additional flexibility to the model, allowing it to better fit the data by adjusting the output independently of the weighted inputs. Biases help to shift the activation function to either the left or right, which can be critical for learning patterns.\n",
    "\n",
    "4. **Activation Function $\\tanh$**: The hyperbolic tangent function, $\\tanh$, is used to add non-linearity to the process, helping the network learn complex patterns. It also squashes the output to be between -1 and 1, which helps in stabilizing the network by controlling the scale of output values.\n",
    "\n",
    "By combining these components, the RNN can maintain a balance between remembering past information and incorporating new inputs, thereby effectively handling sequential data. This equation is repeated at each timestep, continuously updating the hidden state throughout the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351252da-6115-47e1-8e66-90fb14ca5dea",
   "metadata": {},
   "source": [
    "2. **Output Generation**:\n",
    "   The output at each step is generated based on the current state:\n",
    "   $$\n",
    "   o_t = W_{hy} h_t + b_y\n",
    "   $$\n",
    "   where $W_{hy}$ is the weight matrix connecting the state to the output and $b_y$ is the output bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66429181-56d4-4582-979e-7180315b84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461a7258-0c8b-4b35-bc75-38c403b419f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hidden_state(h_prev, x_t, W_hh, W_xh, b_h):\n",
    "    \"\"\"\n",
    "    Update the hidden state for a recurrent neural network.\n",
    "\n",
    "    Parameters:\n",
    "    h_prev (numpy.ndarray): The previous hidden state (vector), shape (hidden_size, 1)\n",
    "    x_t (numpy.ndarray): The current input (vector), shape (input_size, 1)\n",
    "    W_hh (numpy.ndarray): Weight matrix for the hidden-to-hidden connection, shape (hidden_size, hidden_size)\n",
    "    W_xh (numpy.ndarray): Weight matrix for the input-to-hidden connection, shape (hidden_size, input_size)\n",
    "    b_h (numpy.ndarray): Bias vector for the hidden state, shape (hidden_size, 1)\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The updated hidden state (vector), shape (hidden_size, 1)\n",
    "    \"\"\"\n",
    "    z = np.dot(W_hh, h_prev) + np.dot(W_xh, x_t) + b_h\n",
    "    h_t = tanh(z)\n",
    "    return h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8259188-5273-42ad-9a0c-cc1bacf20086",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "The parameters of the model (i.e., $W_{hh}$, $W_{xh}$, $W_{hy}$, $b_h$, and $b_y$) are learned through a process called backpropagation through time (BPTT). BPTT is an extension of the standard backpropagation algorithm used in feedforward networks, modified to handle the sequential nature of RNNs. It involves:\n",
    "\n",
    "- **Forward Pass**: Computing the predicted output and the hidden states for all time steps.\n",
    "- **Loss Calculation**: Evaluating the discrepancy between the predicted output and the actual target output using a loss function, typically Mean Squared Error (MSE) or Cross-Entropy Loss.\n",
    "- **Backward Pass**: Propagating the error back through the network to update the weights and biases to minimize the loss.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "Despite their flexibility and power, RNNs are often challenged by issues like:\n",
    "- **Vanishing Gradient Problem**: During backpropagation, gradients can vanish (become very small) as they are propagated back through time and layers, making learning long-term dependencies practically impossible.\n",
    "- **Exploding Gradient Problem**: Conversely, gradients can also explode (become very large), which may lead to diverging weights during training.\n",
    "\n",
    "To address these issues, more sophisticated variants of RNNs, such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), have been developed.\n",
    "\n",
    "This section has laid out the fundamental architecture of RNNs and their mathematical underpinnings, providing a foundation for understanding how they operate and are implemented in practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d8e26-c9fb-4d0c-a51e-ce95918ec230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

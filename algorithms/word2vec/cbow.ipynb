{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Continuous Bag of Words (Implementation)\n",
    "\n",
    "In this notebook, I implement the Continuous Bag of Words algorithm, which is a word2vec algorithm for calculating word embeddings. First, we must start off with some actual words data to train the CBOW model on. For this, I will use Text8, a text file containing the first 100 MB of cleaned text from Wikipedia. \n",
    "\n",
    "## Motivation\n",
    "\n",
    "One-hot encoding doesn't carry much meaning - word embeddings allow us to capture relationship between words.\n",
    "\n",
    "In CBOW, we predict a target word based on context words."
   ],
   "id": "cbd988072cb80f2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:12:16.763180Z",
     "start_time": "2024-06-01T17:12:16.760273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import torch"
   ],
   "id": "96e4de47e738e7f8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danieltiourine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Prepare data\n",
    "\n",
    "CBOW works by training a model to predict the target word based off of on the context word, allowing us to use the trained parameters as word embeddings. Thus, we must first prepare the words to be inputted into the neural network, so we need to one-hot encode. This entails a few main steps:\n",
    "\n",
    "1. Prepare Corpus: We will use an excerpt from \"Alice in Wonderland\" by Lewis Carroll\n",
    "2. One-hot encode each word in the vocabulary, resulting in a dictionary mapping from each word to a sparse vector.\n",
    "2. Find the context for each word in the vocabulary, resulting in a dictionary mapping from each word to its context words.\n",
    "3. One-hot encode the word-context dictionary using the word-onehot dictionary, resulting in a one-hot encoded word-context dictionary.\n",
    "\n",
    "### Step 1.1: Prepare Corpus"
   ],
   "id": "8e1cce99d104a0ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:12:16.774040Z",
     "start_time": "2024-06-01T17:12:16.770543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_path = 'corpus.txt'\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "print(\"Corpus:\")\n",
    "print(corpus)"
   ],
   "id": "7955dc493bdb948c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:\n",
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
      "\n",
      "There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it was the first time she had ever seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:12:16.788319Z",
     "start_time": "2024-06-01T17:12:16.785170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess words (lowercase, remove punctuation, tokenization)\n",
    "\n",
    "# Sample preprocessed text\n",
    "corpus = corpus.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "corpus = corpus.translate(translator)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_corpus = word_tokenize(corpus)\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = set(tokenized_corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Number of unique words:\", len(vocabulary))"
   ],
   "id": "eef5c7eb2fea8140",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'fortunately', 'burning', 'rabbit', 'of', 'at', 'curiosity', 'whether', 'as', 'when', 'actually', 'could', 'hear', 'for', 'pleasure', 'see', 'on', 'to', 'shall', 'and', 'would', 'watch', 'think', 'by', 'oh', 'eyes', 'just', 'occurred', 'making', 'suddenly', 'own', 'wondered', 'twice', 'started', 'afterwards', 'itself', 'ought', 'with', 'its', 'feel', 'pictures', 'made', 'reading', 'rabbithole', 'took', 'but', 'having', 'is', 'waistcoatpocket', 'this', 'her', 'feet', 'that', 'in', 'out', 'what', 'mind', 'up', 'considering', 'remarkable', 'i', 'without', 'much', 'get', 'white', 'she', 'thought', 'hurried', 'was', 'so', 'use', 'sleepy', 'or', 'trouble', 'well', 'field', 'have', 'pop', 'there', 'book', 'either', 'under', 'worth', 'sister', 'pink', 'down', 'it', 'conversation', 'quite', 'after', 'take', 'across', 'a', 'very', 'hot', 'tired', 'over', 'ever', 'do', 'did', 'then', 'into', 'dear', 'daisies', 'all', 'natural', 'peeped', 'looked', 'no', 'ran', 'picking', 'conversations', 'beginning', 'stupid', 'nor', 'bank', 'large', 'nothing', 'getting', 'hedge', 'day', 'way', 'seen', 'say', 'alice', 'the', 'had', 'time', 'seemed', 'close', 'be', 'first', 'chain', 'late', 'sitting', 'daisy', 'once'}\n",
      "Number of unique words: 136\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1.2: Retrieve Word-Context Pairs\n",
    "\n",
    "(Since in CBOW we are trying to predict the center word based on context words, we first need to define which context words correlate to each word. )\n",
    "In addition to encoding each word, we must also retrieve the contexts of each word. We do this by finding the surrounding words of the target word in different sentences, defined by some window size. Let's choose a window size of 4.\n",
    "\n",
    "The below code creates a dictionary where each key is a word and its corresponding object is the context words."
   ],
   "id": "a03dc522abe2cb88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:12:16.793397Z",
     "start_time": "2024-06-01T17:12:16.789961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "window_size = 2\n",
    "def build_word_context_pairs(tokenized_corpus=tokenized_corpus, vocabulary=vocabulary, window_size=window_size):\n",
    "    word_context_pairs = {word: [] for word in vocabulary}\n",
    "    \n",
    "    for index, target_word in enumerate(tokenized_corpus):\n",
    "        if target_word in vocabulary:\n",
    "            start_index = max(index - window_size, 0)\n",
    "            end_index = min(index + window_size + 1, len(tokenized_corpus))\n",
    "            \n",
    "            context_words = [tokenized_corpus[i] for i in range(start_index, end_index) if i != index]\n",
    "            word_context_pairs[target_word].extend(context_words)\n",
    "            \n",
    "    return word_context_pairs\n",
    "    \n",
    "word_context_pairs = build_word_context_pairs()     \n",
    "print(\"Context pairs for 'alice':\", word_context_pairs['alice'])\n",
    "print(\"Context pairs for 'natural':\", word_context_pairs['natural'])\n",
    "print(\"Context pairs for 'conversations':\", word_context_pairs['conversations'])"
   ],
   "id": "d6d9e94d1b29ada5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context pairs for 'alice': ['was', 'beginning', 'book', 'thought', 'without', 'pictures', 'nor', 'did', 'think', 'it', 'hurried', 'on', 'started', 'to']\n",
      "Context pairs for 'natural': ['seemed', 'quite', 'but', 'when']\n",
      "Context pairs for 'conversations': ['pictures', 'or', 'in', 'it']\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1.3: Encode Words\n",
    "Now that we have prepared our vocabulary into words to context pairs, let's encode these words so they be fed into the model. Instead of traditional one-hot-encoding, word2vec typically uses more efficient representation due to the high computational costs associated with handling large vocabularies. In this case, we will use index mapping."
   ],
   "id": "e530b51da766d80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:12:16.796703Z",
     "start_time": "2024-06-01T17:12:16.794122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_index_mapping(vocabulary):\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "word_to_index, index_to_word = create_index_mapping(vocabulary)"
   ],
   "id": "2aff32e82d972935",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1.4: Set up training data\n",
    "Now that we have encoded our word-context pairs, we can use this to set up the training data."
   ],
   "id": "41d43d8bc230d568"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:12:16.801156Z",
     "start_time": "2024-06-01T17:12:16.798572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_training_data(word_context_pairs, word_to_index):\n",
    "    training_data = []\n",
    "    for word, contexts in word_context_pairs.items():\n",
    "        word_idx = word_to_index[word]\n",
    "        context_indices = [word_to_index[context] for context in contexts]\n",
    "        for context_idx in context_indices:\n",
    "            training_data.append((word_idx, context_idx))\n",
    "    return np.array(training_data, dtype='int32')\n",
    "\n",
    "training_data = get_training_data(word_context_pairs, word_to_index)"
   ],
   "id": "bcc91e74dcdf5eb8",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2: Define and Train Model",
   "id": "2c819c8f498b2bc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T17:12:16.803992Z",
     "start_time": "2024-06-01T17:12:16.802080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "first_five_keys = list(encoded_words_to_context.keys())[:5]\n",
    "\n",
    "# Print the keys and their corresponding values\n",
    "for key in first_five_keys:\n",
    "    print(f\"Key: {key}\")\n",
    "    print(\"Values:\")\n",
    "    if encoded_words_to_context[key]:  # Check if there are any values\n",
    "        for value in encoded_words_to_context[key]:\n",
    "            print(value)\n",
    "    else:\n",
    "        print(\"This key has no values or an empty list.\")\n",
    "    print(\"\\n\")  # Print a newline for better separation between entries\n",
    "\"\"\"\n",
    "print(\"\")"
   ],
   "id": "8ee57fa65583dd1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Continuous Bag of Words (Implemented from Scratch)\n",
    "\n",
    "In this notebook, I implement the Continuous Bag of Words algorithm from scratch, which is a word2vec algorithm for calculating word embeddings. First, we must start off with some actual words data to train the CBOW model on. For this, I will use Text8, a text file containing the first 100 MB of cleaned text from Wikipedia. \n",
    "\n",
    "## Motivation\n",
    "\n",
    "One-hot encoding doesn't carry much meaning - word embeddings allow us to capture relationship between words.\n",
    "\n",
    "In CBOW, we predict a target word based on context words."
   ],
   "id": "cbd988072cb80f2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T23:26:20.616982Z",
     "start_time": "2024-05-27T23:26:19.845090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from collections import Counter\n",
    "import json\n",
    "import os"
   ],
   "id": "96e4de47e738e7f8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Prepare data\n",
    "\n",
    "CBOW works by training a model to predict the target word based off of on the context word, allowing us to use the trained parameters as word embeddings. Thus, we must first prepare the words to be inputted into the neural network, so we need to one-hot encode. This entails a few main steps:\n",
    "\n",
    "1. Retrieve a list of words to serve as vocabulary. We will do this by taking the 1000 most frequent words in the text8 dataset.\n",
    "2. One-hot encode each word in the vocabulary, resulting in a dictionary mapping from each word to a sparse vector.\n",
    "2. Find the context for each word in the vocabulary, resulting in a dictionary mapping from each word to its context words.\n",
    "3. One-hot encode the word-context dictionary using the word-onehot dictionary, resulting in a one-hot encoded word-context dictionary.\n",
    "\n",
    "### 1.1 Prepare Vocabulary List"
   ],
   "id": "8e1cce99d104a0ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T23:26:27.205239Z",
     "start_time": "2024-05-27T23:26:25.387062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the text8 dataset\n",
    "text8_dataset = api.load('text8')\n",
    "text8_words = [word for words in text8_dataset for word in words]\n",
    "print(\"Number of words in text8:\", len(text8_words))"
   ],
   "id": "7955dc493bdb948c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 17005207\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since the Text8 dataset is extremely large, we will take a subset of 1,000 words and work with that instead since it is more practical: the dataset will still be large enough to do its purpose of being a good learning experience but not so much where the computer will run into memory issues. We will do this by considering the words that are most frequent. ",
   "id": "1f45ad90b04ef912"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T00:48:52.319102Z",
     "start_time": "2024-05-28T00:48:50.824275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_vocab_size = 1000\n",
    "word_counts = Counter(text8_words)\n",
    "vocab = [word for word, count in word_counts.most_common(max_vocab_size)]\n",
    "print('Vocabulary:')\n",
    "print(vocab[0:10], '...')"
   ],
   "id": "8f368b59da008b2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two'] ...\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 One Hot Encode Words\n",
    "Now that we have prepared a list of words we will use as our vocabulary to generate word embeddings on, let's create a dictionary to one hot encode each of these words. This is so can we feed the words into our model so it can train parameters."
   ],
   "id": "e530b51da766d80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T23:57:20.302895Z",
     "start_time": "2024-05-27T23:57:19.380870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = OneHotEncoder() \n",
    "vocab_array = np.array(vocab).reshape(-1, 1)\n",
    "encoder.fit(vocab_array)\n",
    "words_to_one_hot = {word: encoder.transform([[word]])[0] for word in vocab} "
   ],
   "id": "2aff32e82d972935",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's save the words_to_one_hot dictionary to a json file.",
   "id": "d97d8a202e5f50e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T23:57:25.646060Z",
     "start_time": "2024-05-27T23:57:24.781929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words_to_one_hot_list = {word: one_hot.toarray().tolist()[0] for word, one_hot in words_to_one_hot.items()} # Convert csr_matrix to list before saving to JSON\n",
    "\n",
    "file_path = 'data/words_to_one_hot.json'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(words_to_one_hot_list, file, indent=4)"
   ],
   "id": "c5725026259f6953",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Retrieve Word Contexts\n",
    "\n",
    "(Since in CBOW we are trying to predict the center word based on context words, we first need to define which context words correlate to each word. )\n",
    "In addition to encoding each word, we must also retrieve the contexts of each word. We do this by finding the surrounding words of the target word in different sentences, defined by some window size. Let's choose a window size of 4.\n",
    "\n",
    "The below code creates a dictionary where each key is a word and its corresponding object is the context words."
   ],
   "id": "a03dc522abe2cb88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T23:18:57.282102Z",
     "start_time": "2024-05-27T23:18:05.810153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WINDOW_SIZE = 5\n",
    "    \n",
    "def build_context_dictionary(words, window_size=WINDOW_SIZE):\n",
    "    \"\"\"   \n",
    "    Args:\n",
    "        words (list of str):List containing all the words (vocabulary)\n",
    "        window_size (int): The number of words to consider on each side of the target word.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary where keys are words and values are their corresponding contexts.\n",
    "    \"\"\"\n",
    "    words_to_context = {} \n",
    "    for index, word in enumerate(words):\n",
    "        start_index = max(0, index - window_size)\n",
    "        end_index = min(len(words), index + window_size + 1)       \n",
    "        word_context = words[start_index:index] + words[index + 1:end_index]\n",
    "        \n",
    "        if word in words_to_context:\n",
    "            words_to_context[word].append(word_context)\n",
    "        else:\n",
    "            words_to_context[word] = [word_context]\n",
    "            \n",
    "    return words_to_context\n"
   ],
   "id": "40e91641e63a12eb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "words_to_context = build_context_dictionary(vocab)",
   "id": "bcc91e74dcdf5eb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we did for the words_to_one_hot dictionary, let's save the words_to_context dictionary as a JSON file as well.",
   "id": "822fafc303a76f7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = 'data/words_to_context.json'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(words_to_context, file, indent=4)"
   ],
   "id": "e9326779805b50b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code below uses the words_to_context JSON file to load the words_to_context dictionary (assuming you have saved the words_to_context JSON file previously)",
   "id": "7ba3c973d665290b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T00:09:02.380430Z",
     "start_time": "2024-05-28T00:09:02.264823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if os.path.exists(file_path):\n",
    "        with open(file_path) as file:\n",
    "            words_to_context = json.load(file)"
   ],
   "id": "481d0f681404fc36",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.4 Encode Words-Context Dictionary\n",
    "\n",
    "Now we have two dictionaries:\n",
    "- `words_to_context`: A dictionary where each key is a word and the corresponding value is a list of context words.\n",
    "- `words_to_one_hot`: A dictionary mapping each word to its one-hot encoded vector.\n",
    "\n",
    "Let's use these two dictionaries to finally create a dictionary where both keys and their corresponding context words are one-hot vectors. "
   ],
   "id": "b8fc4b95ffe2fafc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T00:35:48.083158Z",
     "start_time": "2024-05-28T00:35:47.959135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_words_to_one_hot(words_to_context, word_to_one_hot):\n",
    "    \"\"\"\n",
    "    Convert a dictionary of words and their contexts from words to one-hot encoded vectors.\n",
    "\n",
    "    Args:\n",
    "        words_to_context (dict): A dictionary where keys are words and values are lists of context words.\n",
    "        word_to_one_hot (dict): A dictionary mapping words to their corresponding one-hot encoded vectors.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where both keys and their corresponding context words are converted to string representations of one-hot vectors.\n",
    "    \"\"\"\n",
    "    one_hot_dict = {}\n",
    "\n",
    "    for word, contexts in words_to_context.items():\n",
    "        if word in word_to_one_hot:\n",
    "            # Convert the one-hot vector for the key word to a string key\n",
    "            one_hot_word = json.dumps(word_to_one_hot[word].toarray()[0].tolist())\n",
    "            # Convert the one-hot vectors for all context words to string keys\n",
    "            one_hot_contexts = [json.dumps(word_to_one_hot[ctx].toarray()[0].tolist()) for ctx in contexts if ctx in word_to_one_hot]\n",
    "            # Store in the new dictionary using the string representation of the one-hot vector as the key\n",
    "            one_hot_dict[one_hot_word] = one_hot_contexts\n",
    "\n",
    "    return one_hot_dict\n",
    "\n",
    "encoded_words_to_context = convert_words_to_one_hot(words_to_context, words_to_one_hot)"
   ],
   "id": "b7d70367c17970e6",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we did for the other dictionaries, let's save this as a JSON file.",
   "id": "a2ee169924ffbf1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T00:35:52.031760Z",
     "start_time": "2024-05-28T00:35:51.988358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = 'data/encoded_words_to_context.json'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(encoded_words_to_context, file, indent=4)"
   ],
   "id": "47686d38d8d90ffb",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.5 Create Usable Dataset \n",
    "\n",
    "Now that we have a dictionary containing the words and their corresponding contexts represented as one-hot vectors, `encoded_words_to_context`, we can use this to finally create a usable dataset to train the model. Since in CBOW we are trying to predict the target word based off of the context words, the features here will be the context words, and the \"label\" will be the target word."
   ],
   "id": "ca69acf7ebdd93fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. ",
   "id": "2c819c8f498b2bc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e0ee51a736d0aafa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this notebook, I implement the Continuous Bag of Words algorithm from scratch, which is a word2vec algorithm for calculating word embeddings. First, we must start off with some actual words data to train the CBOW model on. For this, I will use Text8, a text file containing the first 100 MB of cleaned text from Wikipedia. \n",
    "\n",
    "## Motivation\n",
    "\n",
    "One-hot encoding doesn't carry much meaning - word embeddings allow us to capture relationship between words.\n",
    "\n",
    "In CBOW, we predict a target word based on context words."
   ],
   "id": "cbd988072cb80f2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Prepare data\n",
    "\n",
    "CBOW works by training a model to predict the target word based off of on the context word, allowing us to use the trained parameters as word embeddings. Thus, we must first prepare the words to be inputted into the neural network, so we need to one-hot encode.\n",
    "\n",
    "### 1.1 Prepare a vocabulary list and their corresponding one hot encodings"
   ],
   "id": "8e1cce99d104a0ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:31:24.717839Z",
     "start_time": "2024-05-26T21:31:24.135054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from collections import Counter\n",
    "import json"
   ],
   "id": "96e4de47e738e7f8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:31:27.408700Z",
     "start_time": "2024-05-26T21:31:25.970431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the text8 dataset\n",
    "text8_dataset = api.load('text8')\n",
    "\n",
    "# Convert it into a list of words\n",
    "text8_words = [word for words in text8_dataset for word in words]\n",
    "\n",
    "print(\"Number of words:\", len(text8_words))"
   ],
   "id": "7955dc493bdb948c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 17005207\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since the Text8 dataset is extremely large, we will take a subset of 10,000 words and work with that instead since it is more practical: the dataset will still be large enough to do its purpose of being a good learning experience but not so much where the computer will run into memory issues. We will do this by considering the words that are most frequent. ",
   "id": "1f45ad90b04ef912"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:31:32.988797Z",
     "start_time": "2024-05-26T21:31:30.444329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_counts = Counter(text8_words)\n",
    "\n",
    "def limit_vocabulary(word_counts, max_vocab_size=10000):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        word_counts (Counter): A Counter object from the collections module, containing the frequency of each word in the dataset.\n",
    "        max_vocab_size (int, optional): The maximum number of words to include in the reduced vocabulary. Defaults to 10,000. \n",
    "\n",
    "    Returns:\n",
    "        set: A set of strings, each representing a word. This set contains only the most frequent words as specified by 'max_vocab_size'.\n",
    "\n",
    "    \"\"\"\n",
    "    limited_vocab = {word for word, count in word_counts.most_common(max_vocab_size)}\n",
    "    return limited_vocab\n",
    "\n",
    "limited_vocab = limit_vocabulary(word_counts)\n",
    "\n",
    "limited_text8_words = [word for word in text8_words if word in limited_vocab]\n",
    "print(\"Number of words:\", len(set(limited_text8_words)))"
   ],
   "id": "8f368b59da008b2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 10000\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's one hot encode each of those 10,000 words so that each word correlates to a sparse vector.",
   "id": "e530b51da766d80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:32:59.781070Z",
     "start_time": "2024-05-26T21:31:39.468577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Using dense array for easier handling\n",
    "\n",
    "# Convert limited_vocab to a NumPy array with the correct shape\n",
    "vocab_array = np.array(list(limited_vocab)).reshape(-1, 1)\n",
    "\n",
    "# Fit the encoder\n",
    "encoder.fit(vocab_array)\n",
    "one_hot_dict = {word: encoder.transform([[word]])[0] for word in limited_vocab}"
   ],
   "id": "2aff32e82d972935",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Retrieve Word Contexts\n",
    "Since in CBOW we are trying to predict the center word based on context words, we first need to define which context words correlate to each word. We do this by choosing a window size. Let's use a window size of 5. The below code creates a dictionary where each key is a word and its corresponding object is the context words."
   ],
   "id": "a03dc522abe2cb88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:40:15.198179Z",
     "start_time": "2024-05-26T21:37:36.339936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WINDOW_SIZE = 5\n",
    "    \n",
    "def build_context_dictionary(words, window_size=WINDOW_SIZE):\n",
    "    \"\"\"   \n",
    "    Args:\n",
    "        words (list of str):List containing all the words (vocabulary)\n",
    "        window_size (int): The number of words to consider on each side of the target word.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary where keys are words and values are their corresponding contexts.\n",
    "    \"\"\"\n",
    "    words_to_context = {} \n",
    "    for index, word in enumerate(words):\n",
    "        start_index = max(0, index - window_size)\n",
    "        end_index = min(len(words), index + window_size + 1)       \n",
    "        word_context = words[start_index:index] + words[index + 1:end_index]\n",
    "        \n",
    "        if word in words_to_context:\n",
    "            words_to_context[word].append(word_context)\n",
    "        else:\n",
    "            words_to_context[word] = [word_context]\n",
    "            \n",
    "    return words_to_context\n",
    "\n",
    "context_dict = build_context_dictionary(limited_text8_words)\n",
    "\n",
    "# Save context_dict to json to access it easier later\n",
    "\n",
    "#file_path = 'data/context_dict.json'\n",
    "#with open(file_path, 'w') as file:\n",
    "    #json.dump(context_dict, file, indent=4)"
   ],
   "id": "40e91641e63a12eb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have the context-target pairs ready, we need to encode these words into one-hot vectors so that they can be converted into word embeddings by training the parameters of the model to make a prediction of the target word given the context word. Before we can do this, however, we need to flatten all words and contexts into a single list to ensure they are included in the vocabulary of the `OneHotEncoder`. This is necessary to prepare the data",
   "id": "759d23d55457284c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T20:11:54.332667Z",
     "start_time": "2024-05-26T20:11:48.102659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flatten_words_to_context(words_to_context):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        words_to_context (dict): Dictionary where keys are words and values are their corresponding contexts. \n",
    "\n",
    "    Returns:\n",
    "        list of str: List of all words (vocabulary)\n",
    "\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for word, contexts in words_to_context.items():\n",
    "        all_words.append(word)\n",
    "        for context in contexts:\n",
    "            all_words.extend(context)\n",
    "    return all_words\n",
    "\n",
    "all_words = flatten_words_to_context(context_dict)"
   ],
   "id": "e1a9d15bd6782ec8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have flattened the list, let's one hot encode the words.",
   "id": "4f75966aa4c909cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T20:12:52.677753Z",
     "start_time": "2024-05-26T20:11:54.333673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_encode_words(all_words):\n",
    "    one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "    all_words_array = np.array(all_words).reshape(-1, 1)\n",
    "    one_hot_encoder.fit(all_words_array)\n",
    "    return one_hot_encoder\n",
    "\n",
    "def get_one_hot_vector(one_hot_encoder, word):\n",
    "    return one_hot_encoder.transform([[word]])[0]\n",
    "\n",
    "one_hot_encoder = one_hot_encode_words(all_words)"
   ],
   "id": "67d5b2c9824b4aed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, let's create the one-hot encoded dictionary.",
   "id": "7ed0ea1826102dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-26T20:12:52.682759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_one_hot_dict(words_to_context, one_hot_encoder):\n",
    "    one_hot_dict = {}\n",
    "    for word, contexts in words_to_context.items():\n",
    "        one_hot_word = get_one_hot_vector(one_hot_encoder, word)\n",
    "        one_hot_contexts = [get_one_hot_vector(one_hot_encoder, ctx_word) for context in contexts for ctx_word in context]\n",
    "        one_hot_dict[tuple(one_hot_word)] = one_hot_contexts\n",
    "    return one_hot_dict\n",
    "\n",
    "one_hot_encoded_context_dict = create_one_hot_dict(context_dict, one_hot_encoder)"
   ],
   "id": "d0f2a703e86a2347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " \"\"\"\n",
    "def one_hot_encode_context_dict(words_to_context):\n",
    "    \n",
    "    \n",
    "    \n",
    "    one_hot_encoder = OneHotEncoder(sparse=False)  # Use sparse=False to handle the data more easily\n",
    "\n",
    "    # Collect all unique words from keys and context lists\n",
    "    unique_words = set()\n",
    "    for word, contexts in words_to_context.items():\n",
    "        unique_words.add(word)\n",
    "        unique_words.update(contexts)\n",
    "    \n",
    "    # Fit the OneHotEncoder on all unique words\n",
    "    unique_words = np.array(list(unique_words)).reshape(-1, 1)\n",
    "    one_hot_encoder.fit(unique_words)\n",
    "    \n",
    "    # Create one-hot dictionary for all words\n",
    "    one_hot_words = {word: one_hot_encoder.transform([[word]])[0] \n",
    "                     for word in unique_words.flatten()}\n",
    "    \n",
    "    # Map each word and its contexts to their one-hot vectors\n",
    "    one_hot_words_to_context = {\n",
    "        one_hot_words[word]: [one_hot_words[ctx] for ctx in contexts if ctx in one_hot_words]\n",
    "        for word, contexts in words_to_context.items()\n",
    "    }\n",
    "\n",
    "    return one_hot_words_to_context\n",
    "\n",
    "one_hot_words_to_context = one_hot_encode_context_dict(words_to_context)\n",
    "\"\"\""
   ],
   "id": "65016abcdfac2017",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "# Convert words in words_to_context dictionary to one-hot\n",
    "word_to_one_hot = {word[0]: one_hot_encoder.transform([[word[0]]])\n",
    "                   for word in unique_text8_words}\n",
    "one_hot_words_to_contexts = {}\n",
    "for word, contexts in words_to_context.items():\n",
    "    # Transform each context word to one-hot using the map\n",
    "    one_hot_contexts = [word_to_one_hot[context_word] for context_word in contexts if context_word in word_to_one_hot]\n",
    "    # Assign the one-hot encoded context words to the one-hot encoded target word\n",
    "    one_hot_words_to_contexts[word_to_one_hot[word]] = one_hot_contexts\n",
    "    \"\"\""
   ],
   "id": "b2143b8dbf3c9885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "72e2c8b85045b43c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c29b020b1b629d5c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

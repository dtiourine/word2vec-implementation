{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this notebook, I implement the Continuous Bag of Words algorithm from scratch, which is a word2vec algorithm for calculating word embeddings. First, we must start off with some actual words data to train the CBOW model on. For this, I will use Text8, a text file containing the first 100 MB of cleaned text from Wikipedia. \n",
    "\n",
    "In CBOW, we predict a target word based on context words."
   ],
   "id": "cbd988072cb80f2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T03:35:26.061279Z",
     "start_time": "2024-05-24T03:35:24.438759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the text8 dataset\n",
    "text8_dataset = api.load('text8')\n",
    "\n",
    "# Convert it into a list of words\n",
    "text8_words = [word for words in text8_dataset for word in words]\n",
    "\n",
    "print(text8_words[:100])  # Print the first 100 words to check\n",
    "print(\"Number of words:\", len(text8_words))"
   ],
   "id": "7955dc493bdb948c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n",
      "17005207\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since in CBOW we are trying to predict the center word based on context words, we first need to define which context words correlate to each word. We do this by choosing a window size. Let's use a window size of 5. The below code creates a dictionary where each key is a word and its corresponding object is the context words.",
   "id": "a03dc522abe2cb88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T03:35:49.926701Z",
     "start_time": "2024-05-24T03:35:30.590746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words_to_context = {}\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "for index, word in enumerate(text8_words):\n",
    "    # Calculate start and end indices for context\n",
    "    start_index = max(0, index - WINDOW_SIZE)\n",
    "    end_index = min(len(text8_words), index + WINDOW_SIZE + 1)\n",
    "\n",
    "    # Get context words excluding the target word itself\n",
    "    word_context = text8_words[start_index:index] + text8_words[index + 1:end_index]\n",
    "\n",
    "    # Append context words to the list for this word in the dictionary\n",
    "    if word in words_to_context:\n",
    "        words_to_context[word].append(word_context)\n",
    "    else:\n",
    "        words_to_context[word] = [word_context]"
   ],
   "id": "40e91641e63a12eb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have the context-target pairs ready, we need to encode these words into one-hot vectors so that they can be converted into word embeddings by training the parameters of the model to make a prediction of the target word given the context word. The code below one-hot encodes our words.",
   "id": "759d23d55457284c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T04:15:45.466053Z",
     "start_time": "2024-05-24T04:15:44.037262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# One-hot encode vocab\n",
    "one_hot_encoder = OneHotEncoder() \n",
    "unique_text8_words = np.array(list(set(text8_words))).reshape(-1, 1)\n",
    "one_hot_encoder.fit(unique_text8_words)\n",
    "one_hot_encoder.transform(unique_text8_words)\n",
    "unique_text8_words"
   ],
   "id": "911640208598d5aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['georgy'],\n",
       "       ['mwanza'],\n",
       "       ['nerezi'],\n",
       "       ...,\n",
       "       ['duveen'],\n",
       "       ['lecardo'],\n",
       "       ['kolchan']], dtype='<U100')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T03:46:17.569786Z",
     "start_time": "2024-05-24T03:46:17.353323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert words in words_to_context dictionary to one-hot\n",
    "word_to_one_hot = {word[0]: one_hot_encoder.transform([[word[0]]])\n",
    "                   for word in unique_text8_words}\n",
    "one_hot_words_to_contexts = {}\n",
    "for word, contexts in words_to_context.items():\n",
    "    # Transform each context word to one-hot using the map\n",
    "    one_hot_contexts = [word_to_one_hot[context_word] for context_word in contexts if context_word in word_to_one_hot]\n",
    "    # Assign the one-hot encoded context words to the one-hot encoded target word\n",
    "    one_hot_words_to_contexts[word_to_one_hot[word]] = one_hot_contexts"
   ],
   "id": "b2143b8dbf3c9885",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word, contexts \u001B[38;5;129;01min\u001B[39;00m words_to_context\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     11\u001B[0m     word_vector \u001B[38;5;241m=\u001B[39m word_to_one_hot(word, word_to_index, vocab_size)\n\u001B[0;32m---> 12\u001B[0m     context_vectors \u001B[38;5;241m=\u001B[39m [\u001B[43mword_to_one_hot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext_word\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_to_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m context_word \u001B[38;5;129;01min\u001B[39;00m contexts]\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# Now `word_vector` and `context_vectors` are ready for use in training\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[8], line 5\u001B[0m, in \u001B[0;36mword_to_one_hot\u001B[0;34m(word, word_to_index, vocab_size)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mword_to_one_hot\u001B[39m(word, word_to_index, vocab_size):\n\u001B[1;32m      4\u001B[0m     one_hot_vector \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(vocab_size)\n\u001B[0;32m----> 5\u001B[0m     one_hot_vector[\u001B[43mword_to_index\u001B[49m\u001B[43m[\u001B[49m\u001B[43mword\u001B[49m\u001B[43m]\u001B[49m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m one_hot_vector\n",
      "\u001B[0;31mTypeError\u001B[0m: unhashable type: 'list'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T04:09:16.694212Z",
     "start_time": "2024-05-24T04:09:16.691009Z"
    }
   },
   "cell_type": "code",
   "source": "print('test')",
   "id": "72e2c8b85045b43c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c29b020b1b629d5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
